{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "FfG3xQvgyHx2"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we load and process data using the `SarcasmDataset` class. To create a `SarcasmDataset`, input the path of the data csv file and the tokenizer. Later use pytorch to crate a dataloader for the dataset (in the main script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_len, target_col_names=['tweet', 'sarcastic']):\n",
    "        ''' \n",
    "        data_path: path to csv file\n",
    "        tokenizer: tokenizer to use, likely load from AutoTokenizer\n",
    "        max_len: max length of input sequence\n",
    "        '''\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = self.load_data(target_col_names)\n",
    "\n",
    "    def load_data(self, target_col_names):\n",
    "        # use pandas to read csv file\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        # only need certain columns\n",
    "        df = df[target_col_names]\n",
    "        # replace nan with empty string\n",
    "        df = df.fillna('')\n",
    "        # convert to np array\n",
    "        data = df.values\n",
    "        # convert posible emoji to text\n",
    "        data = [[emoji.demojize(text), label] for text, label in data]\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Convert text to tokens, add special tokens, and create attention mask\n",
    "        return: input_ids, attention_mask, label\n",
    "        '''\n",
    "        text, label = self.data[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        input_ids = tokens['input_ids']\n",
    "        attention_mask = tokens['attention_mask']\n",
    "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In this section, we define the model. We will use the ensemble method, which would use multiple models and combine their outputs to get the final prediction.\n",
    "\n",
    "The pretrained models are defined in `PretrainedModelPlus` class, which can take in any pretrained model and add a hidden layer and output layer on top of it. \n",
    "\n",
    "### Ensembling\n",
    "\n",
    "The models are trained separately and the outputs are combined using combined probability. This is implemented in the predict function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedModelPlus(nn.Module):\n",
    "    def __init__(self, pretrained_model, output_size, linear_layer_size):\n",
    "        super(PretrainedModelPlus, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        # Add a linear layer on top of the pretrained model\n",
    "        self.linear = nn.Linear(self.pretrained_model.config.hidden_size, linear_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(linear_layer_size, output_size)\n",
    "        # Add a sigmoid layer to get the probabilities\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Define the loss function\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, labels):\n",
    "        pretrained_outputs = self.pretrained_model(input_ids=x, attention_mask=attention_mask, \n",
    "                                                   return_dict=True, output_hidden_states=True)\n",
    "\n",
    "        hidden_states = torch.stack(pretrained_outputs[\"hidden_states\"])\n",
    "        cat_hidden_states = torch.cat([hidden_states[i] for i in [-1, -2, -3, -4]], dim=1)\n",
    "        first_token = cat_hidden_states[:, 0, :]\n",
    "\n",
    "        linear_outputs = self.linear(first_token)\n",
    "        activation_outputs = self.relu(linear_outputs)\n",
    "        output = self.linear2(activation_outputs)\n",
    "        probs = self.sigmoid(output)\n",
    "        loss = self.loss(probs.view(-1), labels.float())\n",
    "        return loss, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=3, learning_rate=1e-5):\n",
    "    ''' Train a model\n",
    "    model: model to train\n",
    "    dataloader: data loader to use\n",
    "    epochs: number of epochs to train\n",
    "    learning_rate: learning rate to use\n",
    "    return: trained model\n",
    "    '''\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        num_tp = 0\n",
    "        num_fp = 0\n",
    "        num_tn = 0\n",
    "        num_fn = 0\n",
    "\n",
    "        model.train()\n",
    "        for input_ids, attention_mask, labels in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            loss, probs = model(input_ids, attention_mask, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = torch.round(probs)\n",
    "            # compare to labels and update tp, fp, tn, fn\n",
    "            num_tp += ((preds == 1) & (labels == 1)).sum().item()\n",
    "            num_fp += ((preds == 1) & (labels == 0)).sum().item()\n",
    "            num_tn += ((preds == 0) & (labels == 0)).sum().item()\n",
    "            num_fn += ((preds == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "        accuracy = (num_tp + num_tn) / (num_tp + num_fp + num_tn + num_fn)\n",
    "        f1 = 2 * (num_tp / (2 * num_tp + num_fp + num_fn))\n",
    "\n",
    "        # print out stats\n",
    "        print(f'Epoch: {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.4f} | F1: {f1:.4f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(models, data_loaders):\n",
    "    ''' Combine the predictions of models\n",
    "    models: models to use\n",
    "    data_loaders: data loaders to use\n",
    "    return: list of predictions\n",
    "    '''\n",
    "    all_models_probs = []\n",
    "    for data_loader, model in zip(data_loaders, models):\n",
    "        probs = []\n",
    "        for input_ids, attention_mask, labels in tqdm(data_loader):\n",
    "            _, prob = model(input_ids, attention_mask, labels)\n",
    "            prob = prob.detach().numpy()[0][0]\n",
    "            probs.append(prob)\n",
    "        all_models_probs.append(probs)\n",
    "    ensemble_probs = np.array(all_models_probs).mean(axis=0)\n",
    "    preds = [1 if prob > 0.5 else 0 for prob in ensemble_probs]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_from_test_file(models, model_names, test_file, output_file=\"output.csv\"):\n",
    "    ''' Generate predictions from a file\n",
    "    models: list of pretrained models\n",
    "    model_names: list of names of models to use\n",
    "    test_file: file to use for testing\n",
    "    output_file: file to save the predictions\n",
    "    '''\n",
    "    tokenizers = [AutoTokenizer.from_pretrained(model_name) for model_name in model_names]\n",
    "    test_datasets = [SarcasmDataset(test_file, tokenizer, max_len=128, target_col_names=['text', 'sarcastic']) for tokenizer in tokenizers]\n",
    "    test_dataloaders = [torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False) for test_dataset in test_datasets]\n",
    "\n",
    "    preds = predict(models, test_dataloaders)\n",
    "    df = pd.read_csv(test_file)\n",
    "    df['sarcastic'] = preds\n",
    "    df = df[['text', 'sarcastic']]\n",
    "    df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Matrics\n",
    "\n",
    "We use the f1 score as the evaluation matrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5fJMx7En68GD"
   },
   "outputs": [],
   "source": [
    "def evaluate_f1(test_file, model_generated_file):\n",
    "  '''\n",
    "  Inputs a test file and file generated by the model and returns the f1 score using f1_score from sklearn.metrics\n",
    "  :param test_file: csv of shape(num_samples, num_classifications)\n",
    "  :param model_generated_file: csv of shape(num_samples, num_classifications)\n",
    "  :return: f1_score of test_file and model_generated_file of shape(1)\n",
    "  '''\n",
    "  df1 = pd.read_csv(test_file)\n",
    "  df2 = pd.read_csv(model_generated_file)\n",
    "  arr1 = df1['sarcastic'].to_numpy()\n",
    "  arr2 = df2['sarcastic'].to_numpy()\n",
    "\n",
    "  return f1_score(arr1, arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script\n",
    "\n",
    "**Instructions for running the main script:**\n",
    "\n",
    "1. Download the data from [here](https://github.com/iabufarha/iSarcasmEval).\n",
    "\n",
    "2. Create the dataset and dataloader for each of the models.\n",
    "\n",
    "3. Initialize the three models.\n",
    "\n",
    "4. a. Train the three models OR <br>\n",
    "   b. Load the three previously trained and fine tuned models.\n",
    "\n",
    "4. Predict and evaluate f1 score on test set for each individual model.\n",
    "\n",
    "5. Predict and evaluate f1 score on test set for ensemble of models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/junyu/miniforge3/envs/nlp/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Create dataset #1\n",
    "dataset_bertweet = SarcasmDataset(data_path='iSarcasmEval/train/train.En.csv',\n",
    "                                   tokenizer=AutoTokenizer.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis'),\n",
    "                                   max_len=128)\n",
    "# Create data loader #1\n",
    "dataloader_bertweet = torch.utils.data.DataLoader(dataset_bertweet, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset #2\n",
    "dataset_bertweet_irony = SarcasmDataset(data_path='iSarcasmEval/train/train.En.csv',\n",
    "                                        tokenizer=AutoTokenizer.from_pretrained('pysentimiento/bertweet-irony'),\n",
    "                                        max_len=128)\n",
    "# Create data loader #2\n",
    "dataloader_bertweet_irony = torch.utils.data.DataLoader(dataset_bertweet_irony, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset #3\n",
    "dataset_bertweet_c = SarcasmDataset(data_path='iSarcasmEval/train/train.En.csv',\n",
    "                                            tokenizer=AutoTokenizer.from_pretrained('cardiffnlp/bertweet-base-irony'),    \n",
    "                                            max_len=128)\n",
    "# Create data loader #3\n",
    "dataloader_bertweet_c = torch.utils.data.DataLoader(dataset_bertweet_c, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sanity checks\n",
    "assert len(dataset_bertweet) == 3468\n",
    "assert len(dataset_bertweet_irony) == 3468\n",
    "assert len(dataset_bertweet_c) == 3468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model 1 architecture\n",
    "model1 = PretrainedModelPlus(pretrained_model=AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis'), output_size=1, linear_layer_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model 2 architecture\n",
    "model2 = PretrainedModelPlus(pretrained_model=AutoModel.from_pretrained('pysentimiento/bertweet-irony'), output_size=1, linear_layer_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model 3 architecture\n",
    "model3 = PretrainedModelPlus(pretrained_model=AutoModel.from_pretrained('cardiffnlp/bertweet-base-irony'), output_size=1, linear_layer_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc5ec13224c44fbbe865983063864f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Loss: 0.3089 | Accuracy: 0.7447 | F1: 0.0180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6584b0cf2be74ce0b868ef889e68c474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10 | Loss: 0.5727 | Accuracy: 0.6972 | F1: 0.1683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013d04d7d16940a78ed17044e1788bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10 | Loss: 0.3584 | Accuracy: 0.6619 | F1: 0.2333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80adc28c309649aeab7a4e04e65ac044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10 | Loss: 0.3926 | Accuracy: 0.6527 | F1: 0.2445\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b00e915f03a4b5da31e1880476e44af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10 | Loss: 0.0951 | Accuracy: 0.6442 | F1: 0.2453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b51ddf6de5a40fca69dc796879d234b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10 | Loss: 0.0721 | Accuracy: 0.6483 | F1: 0.2656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d88689dce5546eeb8416f1d2320fc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10 | Loss: 0.5480 | Accuracy: 0.6451 | F1: 0.2656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b08f8ffaa35430585d8da0236bbbf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10 | Loss: 0.0594 | Accuracy: 0.6444 | F1: 0.2611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58b07407f23476899d3899a2f8099ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10 | Loss: 0.0462 | Accuracy: 0.6411 | F1: 0.2625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446af529a7da4bf898da3ec65a61a4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10 | Loss: 0.0915 | Accuracy: 0.6409 | F1: 0.2595\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4A.\n",
    "We used this to fine tune model 1. Run this to train model 1.\n",
    "\n",
    "Accuracy metric decreases because less non-sarcastic tweets are being labeled, but more sarcastic tweets are \n",
    "labeled correctly. This works for our overall model because we are trying to detect this sarcasm, indicated by \n",
    "the increasing f1 score.\n",
    "'''\n",
    "model1 = train(model1, dataloader_bertweet, epochs=10, learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615be4ec3b99495abd1d1b1fe0505d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Loss: 0.6022 | Accuracy: 0.7222 | F1: 0.0940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5267ac2c2b64850a4e52c82e1de4f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10 | Loss: 0.3108 | Accuracy: 0.6698 | F1: 0.2168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3bac726f114ce68b7ca3c766a91fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10 | Loss: 0.3952 | Accuracy: 0.6530 | F1: 0.2432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064feefb7ab04426b13910501a8fc95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10 | Loss: 0.2992 | Accuracy: 0.6488 | F1: 0.2488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4259c995799b434ba244f9128ddae3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10 | Loss: 0.1148 | Accuracy: 0.6451 | F1: 0.2539\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694c77be894c4e44b2920a13b112a485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10 | Loss: 0.1430 | Accuracy: 0.6449 | F1: 0.2528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2f51a565b449b5b73c4b25a0005415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10 | Loss: 0.4921 | Accuracy: 0.6460 | F1: 0.2493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df197fbe8fc34d37947dbd438a722d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10 | Loss: 0.2663 | Accuracy: 0.6440 | F1: 0.2513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1be3c1055543efa400c04c68058867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10 | Loss: 0.0670 | Accuracy: 0.6437 | F1: 0.2579\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccbc31df3a842208588e6860bcbeea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10 | Loss: 0.0474 | Accuracy: 0.6457 | F1: 0.2647\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4A.\n",
    "We used this to fine tune model 2. Run this to train model 2.\n",
    "\n",
    "Accuracy metric decreases because less non-sarcastic tweets are being labeled, but more sarcastic tweets are \n",
    "labeled correctly. This works for our overall model because we are trying to detect this sarcasm, indicated by \n",
    "the increasing f1 score.\n",
    "'''\n",
    "model2 = train(model2, dataloader_bertweet_irony, epochs=10, learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07f84f6821b49778ae873615a842deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Loss: 0.6016 | Accuracy: 0.7240 | F1: 0.0854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf62d0b75434df1ae89717a0b7af452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10 | Loss: 0.3997 | Accuracy: 0.6746 | F1: 0.2039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a41fba863104ea98fea57f96b0d3d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10 | Loss: 0.2911 | Accuracy: 0.6617 | F1: 0.2238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd25ea107d8a4f82afa1076a17fcefef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10 | Loss: 0.2739 | Accuracy: 0.6546 | F1: 0.2381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b1ed84c2f64548b5bf23755f35ce22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10 | Loss: 0.2856 | Accuracy: 0.6552 | F1: 0.2488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c0ca3cbf104ff6b76489fe5ab1b4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10 | Loss: 0.1053 | Accuracy: 0.6538 | F1: 0.2499\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efb9db9d20d4b3c80e334e5326dc106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10 | Loss: 0.0865 | Accuracy: 0.6467 | F1: 0.2503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad268ec02d704ee1bef768b0b918aca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10 | Loss: 0.3014 | Accuracy: 0.6517 | F1: 0.2591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35728475e4540c5ade2c43c1c1fe7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10 | Loss: 0.0591 | Accuracy: 0.6495 | F1: 0.2557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c72f07cc41049fda23bf68afa78e50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10 | Loss: 0.0529 | Accuracy: 0.6479 | F1: 0.2593\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4A.\n",
    "We used this to fine tune model 3. Run this to train model 3.\n",
    "\n",
    "Accuracy metric decreases because less non-sarcastic tweets are being labeled, but more sarcastic tweets are \n",
    "labeled correctly. This works for our overall model because we are trying to detect this sarcasm, indicated by \n",
    "the increasing f1 score.\n",
    "'''\n",
    "model3 = train(model3, dataloader_bertweet_c, epochs=10, learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4B.\n",
    "We used this cell to load our previously fine tuned model 2. Run this if you have and wish to use a\n",
    "pretrained model 1.\n",
    "'''\n",
    "# load model1 if it exists\n",
    "if os.path.exists('model1.pt'):\n",
    "    model1.load_state_dict(torch.load('model1.pt'))\n",
    "    print('Loaded model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pysentimiento/bertweet-irony were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at pysentimiento/bertweet-irony and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4B.\n",
    "We used this cell to load our previously fine tuned model 2. Run this if you have and wish to use\n",
    "a pretrained model 2.\n",
    "'''\n",
    "# load model2 if it exists\n",
    "if os.path.exists('model2.pt'):\n",
    "    model2.load_state_dict(torch.load('model2.pt'))\n",
    "    print('Loaded model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/bertweet-base-irony were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/bertweet-base-irony and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4B.\n",
    "We used this cell to load our fine tuned model 3. Run this if you have and wish to use\n",
    "a pretrained model 3.\n",
    "'''\n",
    "# load model3 if it exists\n",
    "if os.path.exists('model3.pt'):\n",
    "    model3.load_state_dict(torch.load('model3.pt'))\n",
    "    print('Loaded model3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd6549bb0984e01961f75bb57dfcd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pred_from_test_file([model1],\n",
    "                             ['finiteautomata/bertweet-base-sentiment-analysis'],    \n",
    "                             'iSarcasmEval/test/task_A_En_test.csv', 'output-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f32fd520d7248fca7e9fee29802c179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pred_from_test_file([model2],\n",
    "                            ['pysentimiento/bertweet-irony'],\n",
    "                            'iSarcasmEval/test/task_A_En_test.csv', 'output-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5078d0c7f75425299ff44708de30e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pred_from_test_file([model3],\n",
    "                            ['cardiffnlp/bertweet-base-irony'],\n",
    "                            'iSarcasmEval/test/task_A_En_test.csv', 'output-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47791164658634533"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1('iSarcasmEval/test/task_A_En_test.csv', 'output-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4771784232365145"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1('iSarcasmEval/test/task_A_En_test.csv', 'output-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46630236794171215"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1('iSarcasmEval/test/task_A_En_test.csv', 'output-3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b053dae1f34009beb7acc45a17a6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d31da8a03c74589897306dd6c114ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63bee3e85734b13a26ae384b76ea14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pred_from_test_file([model1, model2, model3],\n",
    "                                ['finiteautomata/bertweet-base-sentiment-analysis', \n",
    "                                'pysentimiento/bertweet-irony',\n",
    "                                'cardiffnlp/bertweet-base-irony'],\n",
    "                                'iSarcasmEval/test/task_A_En_test.csv', 'output-123.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1('iSarcasmEval/test/task_A_En_test.csv', 'output-123.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), 'model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict(), 'model2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3.state_dict(), 'model3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
