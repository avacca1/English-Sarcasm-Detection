{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FfG3xQvgyHx2"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we load and process data using the `SarcasmDataset` class. To create a `SarcasmDataset`, input the path of the data csv file and the tokenizer. Later use pytorch to crate a dataloader for the dataset (in the main script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_len, target_col_names=['tweet', 'sarcastic']):\n",
    "        ''' \n",
    "        data_path: path to csv file\n",
    "        tokenizer: tokenizer to use, likely load from AutoTokenizer\n",
    "        max_len: max length of input sequence\n",
    "        '''\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = self.load_data(target_col_names)\n",
    "\n",
    "    def load_data(self, target_col_names):\n",
    "        # use pandas to read csv file\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        # only need certain columns\n",
    "        df = df[target_col_names]\n",
    "        # replace nan with empty string\n",
    "        df = df.fillna('')\n",
    "        # convert to np array\n",
    "        data = df.values\n",
    "        # process text\n",
    "        data = [[self.process_tweet(text), label] for text, label in data]\n",
    "        return data\n",
    "    \n",
    "    def process_tweet(self, text):\n",
    "        # convert emoji to text\n",
    "        text = emoji.demojize(text)\n",
    "        # if contain user name (word start with @), replace with @USER\n",
    "        text = ' '.join(['@USER' if word.startswith('@') else word for word in text.split()])\n",
    "        # if contian url, replace with HTTPURL\n",
    "        text = ' '.join(['HTTPURL' if word.startswith('http') else word for word in text.split()])\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Convert text to tokens, add special tokens, and create attention mask\n",
    "        return: input_ids, attention_mask, label\n",
    "        '''\n",
    "        text, label = self.data[idx]\n",
    "        # tokenize text\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        # get input ids: numerical representation of tokens, used as input to model\n",
    "        input_ids = tokens['input_ids']\n",
    "        # get attention mask: binary mask to indicate which tokens should be attended to, used as input to model\n",
    "        attention_mask = tokens['attention_mask']\n",
    "        # convert to tensor and return\n",
    "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In this section, we define the model. We will use the ensemble method, which would use multiple models and combine their outputs to get the final prediction.\n",
    "\n",
    "The pretrained models are defined in `PretrainedModelPlus` class, which can take in any pretrained model and add a hidden layer and output layer on top of it. \n",
    "\n",
    "### Ensembling\n",
    "\n",
    "The models are trained separately and the outputs are combined using combined average probability. This is implemented in the predict function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedModelPlus(nn.Module):\n",
    "    def __init__(self, pretrained_model, output_size, linear_layer_size):\n",
    "        super(PretrainedModelPlus, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        # Add linear layer on top of the pretrained model\n",
    "        self.linear = nn.Linear(self.pretrained_model.config.hidden_size, linear_layer_size)\n",
    "        self.linear_final = nn.Linear(linear_layer_size, output_size)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = nn.ReLU()\n",
    "        # Add a sigmoid layer to get the probabilities\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Define the loss function\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask, labels):\n",
    "        pretrained_outputs = self.pretrained_model(input_ids=x, attention_mask=attention_mask, \n",
    "                                                   return_dict=True, output_hidden_states=True)\n",
    "\n",
    "        # Get all hidden states, size: (num_hidden_layers, batch_size, sequence_length, pretrained_hidden_size)\n",
    "        hidden_states = torch.stack(pretrained_outputs[\"hidden_states\"])\n",
    "        # Sum the last 4 layers, size: (batch_size, sequence_length, pretrained_hidden_size)\n",
    "        summed_last_4_layers = torch.sum(hidden_states[-4:], dim=0)\n",
    "        # Use the first token as the output, size: (batch_size, pretrained_hidden_size)\n",
    "        first_token = summed_last_4_layers[:, 0, :]\n",
    "\n",
    "        # linear layer with activation\n",
    "        linear_outputs = self.linear(first_token)\n",
    "        activation_outputs = self.activation(linear_outputs)\n",
    "\n",
    "        # get output of size 1 for classification\n",
    "        output = self.linear_final(activation_outputs)\n",
    "        # turn output into probabilities\n",
    "        probs = self.sigmoid(output)\n",
    "        # calculate loss\n",
    "        loss = self.loss(probs.view(-1), labels.float())\n",
    "        return loss, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=3, learning_rate=1e-5):\n",
    "    ''' Train a model\n",
    "    model: model to train\n",
    "    dataloader: data loader to use\n",
    "    epochs: number of epochs to train\n",
    "    learning_rate: learning rate to use\n",
    "    return: trained model\n",
    "    '''\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        # keep track of tp, fp, tn, fn\n",
    "        num_tp = 0\n",
    "        num_fp = 0\n",
    "        num_tn = 0\n",
    "        num_fn = 0\n",
    "\n",
    "        model.train()\n",
    "        for input_ids, attention_mask, labels in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            loss, probs = model(input_ids, attention_mask, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # get predictions from probabilities\n",
    "            preds = torch.round(probs)\n",
    "            # compare to labels and update tp, fp, tn, fn\n",
    "            num_tp += ((preds == 1) & (labels == 1)).sum().item()\n",
    "            num_fp += ((preds == 1) & (labels == 0)).sum().item()\n",
    "            num_tn += ((preds == 0) & (labels == 0)).sum().item()\n",
    "            num_fn += ((preds == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "        # calculate accuracy and f1\n",
    "        accuracy = (num_tp + num_tn) / (num_tp + num_fp + num_tn + num_fn)\n",
    "        f1 = 2 * (num_tp / (2 * num_tp + num_fp + num_fn))\n",
    "\n",
    "        # print out stats\n",
    "        print(f'Epoch: {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.4f} | F1: {f1:.4f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(models, data_loaders):\n",
    "    ''' Combine the predictions of models\n",
    "    models: models to use\n",
    "    data_loaders: data loaders to use\n",
    "    return: list of predictions\n",
    "    '''\n",
    "    all_models_probs = [] # list of prob by all models\n",
    "\n",
    "    for data_loader, model in zip(data_loaders, models):\n",
    "        probs = [] # probs for each tweet predicted by one model\n",
    "        for input_ids, attention_mask, labels in tqdm(data_loader):\n",
    "            _, prob = model(input_ids, attention_mask, labels)\n",
    "            prob = prob.detach().numpy()[0][0]\n",
    "            probs.append(prob)\n",
    "        all_models_probs.append(probs)\n",
    "\n",
    "    # average the probs\n",
    "    ensemble_probs = np.array(all_models_probs).mean(axis=0)\n",
    "    # set threshold to 0.5, convert to 0 or 1\n",
    "    preds = [1 if prob > 0.5 else 0 for prob in ensemble_probs]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_from_test_file(models, model_names, test_file, output_file=\"output.csv\"):\n",
    "    ''' Generate predictions from a file\n",
    "    models: list of pretrained models\n",
    "    model_names: list of names of models to use\n",
    "    test_file: file to use for testing\n",
    "    output_file: file to save the predictions\n",
    "    '''\n",
    "    # Load the test file and create a data loader\n",
    "    tokenizers = [AutoTokenizer.from_pretrained(model_name) for model_name in model_names]\n",
    "    test_datasets = [SarcasmDataset(test_file, tokenizer, max_len=128, target_col_names=['text', 'sarcastic']) for tokenizer in tokenizers]\n",
    "    test_dataloaders = [torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False) for test_dataset in test_datasets]\n",
    "\n",
    "    # Generate predictions\n",
    "    preds = predict(models, test_dataloaders)\n",
    "\n",
    "    # Use pandas to save the predictions into a csv file\n",
    "    df = pd.read_csv(test_file)\n",
    "    df['sarcastic'] = preds\n",
    "    df = df[['text', 'sarcastic']]\n",
    "    df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Matrics\n",
    "\n",
    "We use the f1 score as the evaluation matrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5fJMx7En68GD"
   },
   "outputs": [],
   "source": [
    "def evaluate_f1(test_file, model_generated_file):\n",
    "  '''\n",
    "  Inputs a test file and file generated by the model and returns the f1 score using f1_score from sklearn.metrics\n",
    "  :param test_file: csv of shape(num_samples, num_classifications)\n",
    "  :param model_generated_file: csv of shape(num_samples, num_classifications)\n",
    "  :return: f1_score of test_file and model_generated_file of shape(1)\n",
    "  '''\n",
    "  # load file into pandas dataframe\n",
    "  df1 = pd.read_csv(test_file)\n",
    "  df2 = pd.read_csv(model_generated_file)\n",
    "  # get the sarcastic column as a numpy array\n",
    "  arr1 = df1['sarcastic'].to_numpy()\n",
    "  arr2 = df2['sarcastic'].to_numpy()\n",
    "\n",
    "  return f1_score(arr1, arr2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script\n",
    "\n",
    "**Instructions for running the main script:**\n",
    "\n",
    "1. Download the data from [here](https://github.com/iabufarha/iSarcasmEval).\n",
    "\n",
    "2. Create the dataset and dataloader for each of the models.\n",
    "\n",
    "3. Initialize the three models.\n",
    "\n",
    "4. a. Train the three models OR <br>\n",
    "   b. Load the three previously trained and continue to fine tune models.\n",
    "\n",
    "4. Predict and evaluate f1 score on test set for each individual model.\n",
    "\n",
    "5. Predict and evaluate f1 score on test set for ensemble of models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset #1\n",
    "dataset_bertweet = SarcasmDataset(data_path='iSarcasmEval/train/train.En.csv',\n",
    "                                   tokenizer=AutoTokenizer.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis'),\n",
    "                                   max_len=128)\n",
    "# Create data loader #1\n",
    "dataloader_bertweet = torch.utils.data.DataLoader(dataset_bertweet, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset #2\n",
    "dataset_bertweet_irony = SarcasmDataset(data_path='iSarcasmEval/train/train.En.csv',\n",
    "                                        tokenizer=AutoTokenizer.from_pretrained('pysentimiento/bertweet-irony'),\n",
    "                                        max_len=128)\n",
    "# Create data loader #2\n",
    "dataloader_bertweet_irony = torch.utils.data.DataLoader(dataset_bertweet_irony, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset #3\n",
    "dataset_bertweet_c = SarcasmDataset(data_path='iSarcasmEval/train/train.En.csv',\n",
    "                                            tokenizer=AutoTokenizer.from_pretrained('cardiffnlp/bertweet-base-irony'),    \n",
    "                                            max_len=128)\n",
    "# Create data loader #3\n",
    "dataloader_bertweet_c = torch.utils.data.DataLoader(dataset_bertweet_c, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sanity checks\n",
    "assert len(dataset_bertweet) == 3468\n",
    "assert len(dataset_bertweet_irony) == 3468\n",
    "assert len(dataset_bertweet_c) == 3468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model 1 architecture\n",
    "model1 = PretrainedModelPlus(pretrained_model=AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis'), output_size=1, linear_layer_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pysentimiento/bertweet-irony were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at pysentimiento/bertweet-irony and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model 2 architecture\n",
    "model2 = PretrainedModelPlus(pretrained_model=AutoModel.from_pretrained('pysentimiento/bertweet-irony'), output_size=1, linear_layer_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/bertweet-base-irony were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/bertweet-base-irony and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model 3 architecture\n",
    "model3 = PretrainedModelPlus(pretrained_model=AutoModel.from_pretrained('cardiffnlp/bertweet-base-irony'), output_size=1, linear_layer_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec16b8ac15444bc9082dcd5c2b24127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 | Loss: 0.6297 | Accuracy: 0.6722 | F1: 0.2028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc62592467741d5b75b7ddcc86ed741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 | Loss: 0.4367 | Accuracy: 0.6593 | F1: 0.2270\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4A.\n",
    "We used this to fine tune model 1. Run this to train model 1.\n",
    "\n",
    "Accuracy metric decreases because less non-sarcastic tweets are being labeled, but more sarcastic tweets are \n",
    "labeled correctly. This works for our overall model because we are trying to detect this sarcasm, indicated by \n",
    "the increasing f1 score.\n",
    "'''\n",
    "model1 = train(model1, dataloader_bertweet, epochs=2, learning_rate=1e-5) # larger learning rate at first\n",
    "model1 = train(model1, dataloader_bertweet, epochs=2, learning_rate=5e-6) # smaller learning rate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d569d2bd1214032a2ca154cc715f34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 | Loss: 0.4382 | Accuracy: 0.7221 | F1: 0.1116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a65ae4b5ed4c0cbead613b050f6ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3 | Loss: 0.2930 | Accuracy: 0.6729 | F1: 0.2107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edc566666fe4cb19363c5f6ce3c5ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3 | Loss: 0.4043 | Accuracy: 0.6640 | F1: 0.2337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f3a11bb89549fd97523e4ed43d5ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 | Loss: 0.1924 | Accuracy: 0.6550 | F1: 0.2422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59a8a503ca74832bb98125b8a91786f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 | Loss: 0.2814 | Accuracy: 0.6491 | F1: 0.2499\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4A.\n",
    "We used this to fine tune model 2. Run this to train model 2.\n",
    "\n",
    "Accuracy metric decreases because less non-sarcastic tweets are being labeled, but more sarcastic tweets are \n",
    "labeled correctly. This works for our overall model because we are trying to detect this sarcasm, indicated by \n",
    "the increasing f1 score.\n",
    "'''\n",
    "model2 = train(model2, dataloader_bertweet_irony, epochs=3, learning_rate=1e-5) # larger learning rate at first\n",
    "model2 = train(model2, dataloader_bertweet_irony, epochs=2, learning_rate=5e-6) # smaller learning rate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb67211600a4fc7a59e6f48848c49a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 | Loss: 0.8157 | Accuracy: 0.7365 | F1: 0.0501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83944e444e9644d28070b42f710b67cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3 | Loss: 0.4830 | Accuracy: 0.6744 | F1: 0.2133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee1ff4ab7154e188180925ff5585ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3 | Loss: 0.8260 | Accuracy: 0.6530 | F1: 0.2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53df750d36c4a06b2bd8b8dd5721ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 | Loss: 0.3888 | Accuracy: 0.6473 | F1: 0.2548\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4A.\n",
    "We used this to fine tune model 3. Run this to train model 3.\n",
    "\n",
    "Accuracy metric decreases because less non-sarcastic tweets are being labeled, but more sarcastic tweets are \n",
    "labeled correctly. This works for our overall model because we are trying to detect this sarcasm, indicated by \n",
    "the increasing f1 score.\n",
    "'''\n",
    "model3 = train(model3, dataloader_bertweet_c, epochs=3, learning_rate=1e-5) # larger learning rate at first\n",
    "model3 = train(model3, dataloader_bertweet_c, epochs=1, learning_rate=5e-6) # smaller learning rate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4B.\n",
    "We used this cell to load our previously fine tuned model 2. Run this if you have and wish to use a\n",
    "pretrained model 1.\n",
    "'''\n",
    "# load model1 if it exists\n",
    "if os.path.exists('model1.pt'):\n",
    "    model1.load_state_dict(torch.load('model1.pt'))\n",
    "    print('Loaded model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4B.\n",
    "We used this cell to load our previously fine tuned model 2. Run this if you have and wish to use\n",
    "a pretrained model 2.\n",
    "'''\n",
    "# load model2 if it exists\n",
    "if os.path.exists('model2.pt'):\n",
    "    model2.load_state_dict(torch.load('model2.pt'))\n",
    "    print('Loaded model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/bertweet-base-irony were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/bertweet-base-irony and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPTION 4B.\n",
    "We used this cell to load our fine tuned model 3. Run this if you have and wish to use\n",
    "a pretrained model 3.\n",
    "'''\n",
    "# load model3 if it exists\n",
    "if os.path.exists('model3.pt'):\n",
    "    model3.load_state_dict(torch.load('model3.pt'))\n",
    "    print('Loaded model3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c14dbd43374dacbad4e5939d9cc272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pred_from_test_file([model1],\n",
    "                             ['finiteautomata/bertweet-base-sentiment-analysis'],    \n",
    "                             'iSarcasmEval/test/task_A_En_test.csv', 'output-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9f5ca91c7f42fa9a4d6b6aa87ef468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pred_from_test_file([model2],\n",
    "                            ['pysentimiento/bertweet-irony'],\n",
    "                            'iSarcasmEval/test/task_A_En_test.csv', 'output-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3566e4494e18465795b282ae402c5de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pred_from_test_file([model3],\n",
    "                            ['cardiffnlp/bertweet-base-irony'],\n",
    "                            'iSarcasmEval/test/task_A_En_test.csv', 'output-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.460431654676259"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1('iSarcasmEval/test/task_A_En_test.csv', 'output-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5148514851485149"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1('iSarcasmEval/test/task_A_En_test.csv', 'output-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4801670146137787"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1('iSarcasmEval/test/task_A_En_test.csv', 'output-3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23787154d4d34578803fbd0112bc28b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3ec56566504318859ccd02fdd7fd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d479204d0e534cfab7646e662c16ef35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pred_from_test_file([model1, model2, model3],\n",
    "                                ['finiteautomata/bertweet-base-sentiment-analysis', \n",
    "                                'pysentimiento/bertweet-irony',\n",
    "                                'cardiffnlp/bertweet-base-irony'],\n",
    "                                'iSarcasmEval/test/task_A_En_test.csv', 'output-123.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.538135593220339"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1('iSarcasmEval/test/task_A_En_test.csv', 'output-123.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), 'model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict(), 'model2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3.state_dict(), 'model3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
