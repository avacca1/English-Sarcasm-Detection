{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FfG3xQvgyHx2"
      },
      "outputs": [],
      "source": [
        "import os, random, sys, copy\n",
        "import torch, torch.nn as nn, numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import emoji"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "In this section, we load and process data using the `SarcasmDataset` class. To create a `SarcasmDataset`, input the path of the data csv file and the tokenizer. Later use pytorch to crate a dataloader for the dataset (in the main script)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SarcasmDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, tokenizer, max_len):\n",
        "        ''' \n",
        "        data_path: path to csv file\n",
        "        tokenizer: tokenizer to use, likely load from AutoTokenizer\n",
        "        max_len: max length of input sequence\n",
        "        '''\n",
        "        self.data_path = data_path\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.data = self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        # use pandas to read csv file\n",
        "        df = pd.read_csv(self.data_path)\n",
        "        # only need the 2nd and 3rd col (text, label)\n",
        "        df = df.iloc[:, 1:3]\n",
        "        # replace nan with empty string\n",
        "        df = df.fillna('')\n",
        "        # convert to np array\n",
        "        data = df.values\n",
        "        # convert posible emoji to text\n",
        "        data = [[emoji.demojize(text), label] for text, label in data]\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Convert text to tokens, add special tokens, and create attention mask\n",
        "        return: input_ids, attention_mask, label\n",
        "        '''\n",
        "        text, label = self.data[idx]\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        if len(tokens) < self.max_len:\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.max_len - len(tokens))]\n",
        "        else:\n",
        "            tokens = tokens[:self.max_len - 1] + ['[SEP]']\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1 if token != '[PAD]' else 0 for token in tokens]\n",
        "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(label)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\n",
        "\n",
        "In this section, we define the model. We will use the ensemble method, which would use multiple models and combine their outputs to get the final prediction. The models we will use are: 1. `Custom LSTM model` 2. `RoBERTa model` 3. `BERT model` (the 2nd and 3rd model are pretained, so can be change in the future to other pretrained models if needed).\n",
        "\n",
        "The custom LSTM model is a simple LSTM model is defined in `CustomLSTM` class. The pretrained models are defined in `PretrainedModelPlus` class, which can take in any pretrained model and add a hidden layer and output layer on top of it. \n",
        "\n",
        "### Architecture\n",
        "\n",
        "#### Custom LSTM\n",
        "\n",
        "- Embedding layer: 50d GloVe embedding\n",
        "- LSTM layer: bidirectional LSTM \n",
        "- Hidden layer: two linear layers with non-linear activation function\n",
        "- Output layer: linear with output size 1\n",
        "\n",
        "#### RoBERTa\n",
        "\n",
        "- RoBERTa model: pretrained RoBERTa model, freezed the weights\n",
        "- Hidden layer: two linear layers with non-linear activation function\n",
        "- Output layer: linear with output size 1\n",
        "\n",
        "#### BERT\n",
        "\n",
        "- BERT model: pretrained BERT model, freezed the weights\n",
        "- Hidden layer: two linear layers with non-linear activation function\n",
        "- Output layer: linear with output size 1\n",
        "\n",
        "\n",
        "### Ensembling\n",
        "\n",
        "The models are trained separately and the outputs are combined using combined probability. This is implemented in the predict function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ-3-SyF0iSo"
      },
      "outputs": [],
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "  def __init__(self, embedding_matrix, lstm_hidden_size=50, num_lstm_layers=1, bidirectional=True, activation='ReLU'):\n",
        "    \"\"\"\n",
        "    Initalizes the overall structure of the Sarcasm Model\n",
        "\n",
        "    param embedding_matrix: matrix of pretrained Glove embeddings (dataset doesn't come with vocab so this is easier, but may require extra processing for emojis)\n",
        "    param lstm_hidden_size: size of the hidden layer of the lstm\n",
        "    param num_lstm_layers: number of lstm layers\n",
        "    param bidirectional: whether the sentence embedding is bidirectional or not\n",
        "    param activation: the final activation functino to be applied\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
        "    self.lstm = nn.LSTM(input_size = embedding_matrix.shape[1],\n",
        "                            hidden_size = lstm_hidden_size,\n",
        "                            num_layers = num_lstm_layers,\n",
        "                            bidirectional = bidirectional,\n",
        "                            batch_first = True)\n",
        "    self.hidden_1 = nn.Linear(lstm_hidden_size * 2, lstm_hidden_size)\n",
        "    self.hidden_2 = nn.Linear(lstm_hidden_size, 1)\n",
        "    self.activation_function = nn.ReLU()\n",
        "\n",
        "  def forward(self, input_batch, input_lengths):\n",
        "    embedded_input = self.embedding(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PretrainedModelPlus(nn.Module):\n",
        "    def __init__(self, pretrained_model, num_classes, linear_layer_size):\n",
        "        super().__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "        # Add a linear layer on top of the pretrained model\n",
        "        self.linear = nn.Linear(self.pretrained_model.config.hidden_size, linear_layer_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(linear_layer_size, num_classes)\n",
        "        # Add a sigmoid layer to get the probabilities\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # Define the loss function\n",
        "        self.loss = nn.BCELoss()\n",
        "        # Define the number of classes\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Freeze the pretrained model\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, attention_mask, labels):\n",
        "        pretrained_outputs = self.pretrained_model(input_ids=x, attention_mask=attention_mask)\n",
        "        linear_outputs = self.linear(pretrained_outputs.pooler_output)\n",
        "        activation_outputs = self.relu(linear_outputs)\n",
        "        output = self.linear2(activation_outputs)\n",
        "        probs = self.sigmoid(output)\n",
        "        loss = self.loss(probs.view(-1), labels.float())\n",
        "        return loss, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trian(model, dataloader, epochs=5, learning_rate=1e-5):\n",
        "    ''' Train a model\n",
        "    model: model to train\n",
        "    dataloader: data loader to use\n",
        "    epochs: number of epochs to train\n",
        "    learning_rate: learning rate to use\n",
        "    return: trained model\n",
        "    '''\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for input_ids, attention_mask, labels in dataloader:\n",
        "            loss, probs = model(input_ids, attention_mask, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model1, model2, model3, data_loader):\n",
        "    ''' Combine the predictions of 3 models\n",
        "    model1, model2, model3: models to use\n",
        "    data_loader: data loader to use\n",
        "    return: list of predictions\n",
        "    '''\n",
        "    preds = []\n",
        "    for input_ids, attention_mask, labels in data_loader:\n",
        "        loss1, probs1 = model1(input_ids, attention_mask, labels)\n",
        "        loss2, probs2 = model2(input_ids, attention_mask, labels)\n",
        "        loss3, probs3 = model3(input_ids, attention_mask, labels)\n",
        "        probs = (probs1 + probs2 + probs3) / 3\n",
        "        pred = 1 if probs > 0.5 else 0\n",
        "        preds.append(pred)\n",
        "    return preds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Matrics\n",
        "\n",
        "We use the f1 score as the evaluation matrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fJMx7En68GD"
      },
      "outputs": [],
      "source": [
        "def evaluate(test_file, model_generated_file):\n",
        "  '''\n",
        "  Inputs a test file and file generated by the model and returns the f1 score using f1_score from sklearn.metrics\n",
        "  :param test_file: csv of shape(num_samples, num_classifications)\n",
        "  :param model_generated_file: csv of shape(num_samples, num_classifications)\n",
        "  :return: f1_score of test_file and model_generated_file of shape(1)\n",
        "  '''\n",
        "  arr1 = np.loadtxt(\"test_file\",delimiter=\",\", dtype=str) # from https://www.geeksforgeeks.org/how-to-read-csv-files-with-numpy/#\n",
        "  arr2 = np.loadtxt(\"model_generated_file\",delimiter=\",\", dtype=str)\n",
        "\n",
        "  return f1_score(arr1, arr2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Script\n",
        "\n",
        "**Instructions for running the main script:**\n",
        "\n",
        "1. Download the data from [here](https://github.com/iabufarha/iSarcasmEval).\n",
        "\n",
        "2. Create the dataset and dataloader for each of the models.\n",
        "\n",
        "3. Create and Train model\n",
        "\n",
        "4. Predict and evaluate f1 score on test set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset #2\n",
        "dataset_roberta = SarcasmDataset(data_path='iSarcasmEval/train/train.En.csv',\n",
        "                                 tokenizer=AutoTokenizer.from_pretrained('roberta-base'),\n",
        "                                 max_len=128)\n",
        "# Create data loader #2\n",
        "dataloader_roberta = torch.utils.data.DataLoader(dataset_roberta, batch_size=32, shuffle=True)\n",
        "\n",
        "# Create dataset #3\n",
        "dataset_bert = SarcasmDataset(data_path='iSarcasmEval/train/train.En.csv',\n",
        "                              tokenizer=AutoTokenizer.from_pretrained('bert-base'),\n",
        "                              max_len=128)\n",
        "# Create data loader #3\n",
        "dataloader_bert = torch.utils.data.DataLoader(dataset_bert, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Some sanity checks\n",
        "assert len(dataset_roberta) == 3468\n",
        "assert len(dataset_bert) == 3468"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGzAWkNwyvIK"
      },
      "outputs": [],
      "source": [
        "# Create models\n",
        "model2 = PretrainedModelPlus(pretrained_model=AutoModel.from_pretrained('roberta-base'), num_classes=1, linear_layer_size=100)\n",
        "model3 = PretrainedModelPlus(pretrained_model=AutoModel.from_pretrained('bert-base'), num_classes=1, linear_layer_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train models\n",
        "model2 = trian(model2, dataloader_roberta, epochs=5, learning_rate=1e-5)\n",
        "model3 = trian(model3, dataloader_bert, epochs=5, learning_rate=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on test set\n",
        "test_dataset = SarcasmDataset(data_path='iSarcasmEval/test/test.En.csv',\n",
        "                                tokenizer=AutoTokenizer.from_pretrained('bert-base'),\n",
        "                                max_len=128)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Get predictions\n",
        "preds = predict(model1, model2, model3, test_dataloader)\n",
        "\n",
        "# F1 score\n",
        "f1_score = evaluate('iSarcasmEval/test/test.En.csv', preds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
